{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from mlagents.envs.environment import UnityEnvironment, BrainParameters\n",
        "import numpy as np\n",
        "from qlearning_agent import *\n",
        "from function_approximators import *\n",
        "import tqdm\n",
        "import pdb\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "execution_count": 37,
      "metadata": {
        "button": false,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Environment"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "env = UnityEnvironment(file_name=\"environment-binaries/FoodCollector\")\n",
        "# env = UnityEnvironment(file_name=None)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:mlagents.envs:\n",
            "'Academy' started successfully!\n",
            "Unity Academy name: Academy\n",
            "        Number of Training Brains : 0\n",
            "        Reset Parameters :\n",
            "\t\t\n",
            "\n"
          ]
        }
      ],
      "execution_count": 51,
      "metadata": {
        "button": false,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Agent"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "env.step()\n",
        "env.reset()\n",
        "for name, brain in env.brains.items():\n",
        "    print(f\"Brain name: {name}\")\n",
        "    parameters = env.brains[name]\n",
        "    print(\"State space size:\\t\", parameters.vector_observation_space_size)\n",
        "    print(\"Action space size:\\t\", parameters.vector_action_space_size)\n",
        "    print(\"Action space type:\\t\", parameters.vector_action_space_type)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Brain name: VisualFoodCollector\n",
            "State space size:\t 0\n",
            "Action space size:\t [3, 3, 3, 2]\n",
            "Action space type:\t discrete\n"
          ]
        }
      ],
      "execution_count": 52,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "actions = itertools.product((0, 1, 2), (0, 1, 2), (0, 1, 2), (0, 1))\n",
        "approximator = LinearFunctionApproximator(70, 1, actions)\n",
        "agent = QLearningAgent(actions, approximator)"
      ],
      "outputs": [],
      "execution_count": 41,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "max_episodes = 100\n",
        "max_timestep = 100"
      ],
      "outputs": [],
      "execution_count": 42,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "info = env.step()"
      ],
      "outputs": [],
      "execution_count": 53,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "info[\"VisualFoodCollector\"].vector_observations"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 57,
          "data": {
            "text/plain": [
              "array([], shape=(4, 0), dtype=float64)"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 57,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "rewards_all_episodes = []\n",
        "for _ in tqdm.tqdm(range(max_episodes)):\n",
        "    rewards_current_episode = 0\n",
        "    info = env.step()[\"PushBlock\"]\n",
        "    for _ in range(max_timestep):\n",
        "        reward = info.rewards[agent]\n",
        "        rewards_current_episode += reward\n",
        "        state = info.vector_observations[agent]\n",
        "        agent.give_feedback(reward, state)\n",
        "        action = agent.get_action(state)\n",
        "        info = env.step(action)[\"PushBlock\"]\n",
        "    rewards_all_episodes.append(rewards_current_episode)\n",
        "    env.reset(train_mode=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/100 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "shapes (211,) and (71,) not aligned: 211 (dim 0) != 71 (dim 0)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-40e96d343683>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_observations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgive_feedback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"PushBlock\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Documents/GitHub/dodgeball/qlearning_agent.py\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mbest_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mftn_approximator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_value\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0mbest_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Documents/GitHub/dodgeball/function_approximators.py\u001b[0m in \u001b[0;36mpredict_q\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mstate_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (211,) and (71,) not aligned: 211 (dim 0) != 71 (dim 0)"
          ]
        }
      ],
      "execution_count": 44,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teardown Environment"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "env.close()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:mlagents.envs:Environment shut down with return code 0.\n"
          ]
        }
      ],
      "execution_count": 58,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "0.15.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}