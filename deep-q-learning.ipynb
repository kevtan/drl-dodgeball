{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "from replay_memory import ReplayMemory\n",
    "import random\n",
    "import pdb\n",
    "import math\n",
    "from mlagents.envs.environment import UnityEnvironment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Network\n",
    "\n",
    "### Playing Atari with Deep Reinforcement Learning\n",
    "https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\n",
    "\n",
    "There are several possible ways of parameterizing Q using a neural network. Since Q maps history-action pairs to scalar estimates of their Q-value, the history and the action have been used as inputs to the neural network by some previous approaches [20, 12]. The main drawback of this type of architecture is that a separate forward pass is required to compute the Q-value of each action, resulting in a cost that scales linearly with the number of actions. We instead use an architecture in which there is a separate output unit for each possible action, and only the state representation is an input to the neural network. The outputs correspond to the predicted Q-values of the individual action for the input state. The main advantage of this type of architecture is the ability to compute Q-values for all possible actions in a given state with only a single forward pass through the network.\n",
    "\n",
    "We now describe the exact architecture used for all seven Atari games. The input to the neural\n",
    "network consists is an 84 × 84 × 4 image produced by φ. The first hidden layer convolves 16 8 × 8\n",
    "filters with stride 4 with the input image and applies a rectifier nonlinearity [10, 18]. The second\n",
    "hidden layer convolves 32 4 × 4 filters with stride 2, again followed by a rectifier nonlinearity. The\n",
    "final hidden layer is fully-connected and consists of 256 rectifier units. The output layer is a fully-connected linear layer with a single output for each valid action. The number of valid actions varied\n",
    "between 4 and 18 on the games we considered. We refer to convolutional networks trained with our\n",
    "approach as Deep Q-Networks (DQN).\n",
    "\n",
    "### Our Networks\n",
    "\n",
    "#### Vector Observations (PushBlock No-Stack Small)\n",
    "* We have 7 ray angles per ray scan. Each ray angle contributes a length 5 sublist containing data of the form [hit_block, hit_goal, hit_wall, hit_anything, distance_if_hit]. This is essentially a one-hot representation of the the objects in the environment.\n",
    "* We have 2 ray scans at different angles, so the agent observes a total of **70 data elements every timestep**.\n",
    "* Vector observation space (size 70).\n",
    "* Action space (size 7): 0, 1, ..., 6.\n",
    "\n",
    "Based on the work of Mnih et al., for our Q-network that uses vector observations (basically pre-extracted features), we are going to have 1 hidden layer (with 100 rectified units) and 1 output layer (with 7 units corresponding to the 7 actions in the action space).\n",
    "\n",
    "#### Visual Observations (PushBlock No-Stack Small)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PushBlockNoStackSmallNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(PushBlockNoStackSmallNetwork, self).__init__()\n",
    "        self.hidden = nn.Linear(70, 100)\n",
    "        self.out = nn.Linear(100, 7)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        temp = F.relu(self.hidden(state))\n",
    "        return self.out(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "### Playing Atari with Deep Reinforcement Learning\n",
    "https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\n",
    "\n",
    "In these experiments, we used the RMSProp algorithm with minibatches of size 32. The behavior\n",
    "policy during training was epsilon-greedy with epsilon annealed linearly from 1 to 0.1 over the first million\n",
    "frames, and fixed at 0.1 thereafter. We trained for a total of 10 million frames and used a replay\n",
    "memory of one million most recent frames.\n",
    "\n",
    "Following previous approaches to playing Atari games, we also use a simple frame-skipping technique [3]. More precisely, the agent sees and selects actions on every kth frame instead of every frame, and its last action is repeated on skipped frames. Since running the emulator forward for one step requires much less computation than having the agent select an action, this technique allows the agent to play roughly k times more games without significantly increasing the runtime. We use k = 4 for all games except Space Invaders where we noticed that using k = 4 makes the lasers invisible because of the period at which they blink. We used k = 3 to make the lasers visible and this change was the only difference in hyperparameter values between any of the games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 1000\n",
    "num_timesteps = 2000\n",
    "discount = 0.99\n",
    "exploration_rate = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vec_obs(brain_info, brain_name):\n",
    "    \"\"\"Extract vector observations from a BrainInfo object.\"\"\"\n",
    "    vec_obs = brain_info[brain_name].vector_observations.flatten()\n",
    "    return torch.from_numpy(vec_obs).float()\n",
    "\n",
    "def extract_reward(brain_info, brain_name, brain_num):\n",
    "    \"\"\"Extract reward from BrainInfo object.\"\"\"\n",
    "    return brain_info[brain_name].rewards[brain_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mlagents.envs:\n",
      "'Academy (1)' started successfully!\n",
      "Unity Academy name: Academy (1)\n",
      "        Number of Training Brains : 0\n",
      "        Reset Parameters :\n",
      "\t\tblock_scale -> 2.0\n",
      "\t\tstatic_friction -> 0.0\n",
      "\t\tdynamic_friction -> 0.0\n",
      "\t\tblock_drag -> 0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"environment-binaries/PushBlock-no-stack-small.app\")\n",
    "brain_name = \"PushBlock\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [0, 1, 2, 3, 4, 5, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PushBlockNoStackSmallNetwork(\n",
      "  (hidden): Linear(in_features=70, out_features=100, bias=True)\n",
      "  (out): Linear(in_features=100, out_features=7, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "qnet = PushBlockNoStackSmallNetwork()\n",
    "print(qnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(qnet.parameters(), 0.1, weight_decay=0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1000 [00:18<2:24:52,  8.71s/it]"
     ]
    }
   ],
   "source": [
    "for _ in tqdm.tqdm(range(num_episodes)):\n",
    "    # initialize start state\n",
    "    braininfos = env.step()\n",
    "    for _ in range(num_timesteps):\n",
    "        # train all brains in environment\n",
    "        for i, brain in enumerate(braininfos):\n",
    "            # choose action\n",
    "            state = extract_vec_obs(braininfos, brain)\n",
    "            if random.random() < exploration_rate:\n",
    "                action = random.choice(actions)\n",
    "            else:\n",
    "                q_values = qnet(state)\n",
    "                action = torch.argmax(q_values).item()\n",
    "            # execute action\n",
    "            brain_infos = env.step(action)\n",
    "            # calculate gradients\n",
    "            prediction = qnet(state)[action]\n",
    "            next_state = extract_vec_obs(brain_infos, brain)\n",
    "            reward = extract_reward(brain_infos, brain_name, i)\n",
    "            target = reward + discount * max(qnet(next_state))\n",
    "            loss = (target - prediction) ** 2\n",
    "            loss.backward()\n",
    "            # update network parameters\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnityEnvironmentException",
     "evalue": "No Unity environment is loaded.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnityEnvironmentException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-1baceacf4cb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/mlagents/envs/environment.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    570\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mUnityEnvironmentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No Unity environment is loaded.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnityEnvironmentException\u001b[0m: No Unity environment is loaded."
     ]
    }
   ],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Evaluation Metrics\n",
    "\n",
    "### Playing Atari with Deep Reinforcement Learning\n",
    "https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\n",
    "\n",
    "In supervised learning, one can easily track the performance of a model during training by evaluating\n",
    "it on the training and validation sets. In reinforcement learning, however, accurately evaluating the\n",
    "progress of an agent during training can be challenging. Since our evaluation metric, as suggested\n",
    "by [3], is the total reward the agent collects in an episode or game averaged over a number of\n",
    "games, we periodically compute it during training. **The average total reward metric tends to be very\n",
    "noisy** because small changes to the weights of a policy can lead to large changes in the distribution of\n",
    "states the policy visits . The leftmost two plots in figure 2 show how the average total reward evolves\n",
    "during training on the games Seaquest and Breakout. Both averaged reward plots are indeed quite\n",
    "noisy, giving one the impression that the learning algorithm is not making steady progress. **Another,\n",
    "more stable, metric is the policy’s estimated action-value function Q**, which provides an estimate of\n",
    "how much discounted reward the agent can obtain by following its policy from any given state. We\n",
    "collect a fixed set of states by running a random policy before training starts and track the average\n",
    "of the maximum2 predicted Q for these states. The two rightmost plots in figure 2 show that average\n",
    "predicted Q increases much more smoothly than the average total reward obtained by the agent and\n",
    "plotting the same metrics on the other five games produces similarly smooth curves. In addition\n",
    "to seeing relatively smooth improvement to predicted Q during training we did not experience any\n",
    "divergence issues in any of our experiments. This suggests that, despite lacking any theoretical\n",
    "convergence guarantees, our method is able to train large neural networks using a reinforcement\n",
    "learning signal and stochastic gradient descent in a stable manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
